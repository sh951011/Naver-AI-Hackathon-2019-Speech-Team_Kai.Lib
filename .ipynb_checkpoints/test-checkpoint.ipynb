{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2019-10-29 14:46:15,451 <ipython-input-1-01010a3e56a3>:81 - <module>()] start\n",
      "train1\n",
      "train1-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SooHwanKim\\Desktop\\수환\\python_workspace\\Kai.Lib_75.3\\Naver-Hackathon-2019-Speech-Team_Kai.Lib\\loader\\loader.py\", line 150, in run\n",
      "    items.append(self.dataset.getitem(self.index))\n",
      "  File \"C:\\Users\\SooHwanKim\\Desktop\\수환\\python_workspace\\Kai.Lib_75.3\\Naver-Hackathon-2019-Speech-Team_Kai.Lib\\loader\\loader.py\", line 82, in getitem\n",
      "    script = get_script(self.script_paths[idx], self.bos_id, self.eos_id, self.target_dict)\n",
      "  File \"C:\\Users\\SooHwanKim\\Desktop\\수환\\python_workspace\\Kai.Lib_75.3\\Naver-Hackathon-2019-Speech-Team_Kai.Lib\\loader\\loader.py\", line 42, in get_script\n",
      "    script = target_dict[key]\n",
      "KeyError: 'train_data\\\\41_0601_211_0_07930_02'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import queue\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from funcCall import *\n",
    "from loader import label_loader\n",
    "from models import EncoderRNN, DecoderRNN, Seq2seq\n",
    "from hyperParams import HyperParams\n",
    "\n",
    "global target_dict\n",
    "global char2index\n",
    "global index2char\n",
    "global SOS_token\n",
    "global EOS_token\n",
    "global PAD_token\n",
    "DATASET_PATH = './train/'\n",
    "target_dict = dict()\n",
    "\n",
    "hyper_p = HyperParams()\n",
    "#hyper_p.input_params()\n",
    "char2index, index2char = label_loader.load_label('./hackathon.labels')\n",
    "SOS_token = char2index['<s>']\n",
    "EOS_token = char2index['</s>']\n",
    "PAD_token = char2index['_']\n",
    "\n",
    "random.seed(hyper_p.seed)\n",
    "torch.manual_seed(hyper_p.seed)\n",
    "torch.cuda.manual_seed_all(hyper_p.seed)\n",
    "cuda = not hyper_p.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "feature_size = 40  # MFCC n_mfcc = 40이라 40\n",
    "\n",
    "enc = EncoderRNN(feature_size, hyper_p.hidden_size ,\n",
    "                 input_dropout_p = hyper_p.dropout, dropout_p = hyper_p.dropout,\n",
    "                 n_layers = hyper_p.layer_size,\n",
    "                 bidirectional = hyper_p.bidirectional, rnn_cell = 'gru', variable_lengths = False)\n",
    "\n",
    "dec = DecoderRNN(len(char2index), hyper_p.max_len, hyper_p.hidden_size * (2 if hyper_p.bidirectional else 1),\n",
    "                 SOS_token, EOS_token,\n",
    "                 n_layers = hyper_p.layer_size, rnn_cell = 'gru', bidirectional = hyper_p.bidirectional,\n",
    "                 input_dropout_p = hyper_p.dropout, dropout_p = hyper_p.dropout, use_attention = hyper_p.attention)\n",
    "\n",
    "model = Seq2seq(enc, dec)\n",
    "model.flatten_parameters()\n",
    "model = nn.DataParallel(model).to(device) # 병렬처리 부분인 듯\n",
    "\n",
    "# Adam Algorithm\n",
    "optimizer = optim.Adam(model.module.parameters(), lr = hyper_p.lr)\n",
    "# CrossEntropy로 loss 계산\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=PAD_token).to(device)\n",
    "\n",
    "# 데이터 로드 start\n",
    "data_list = os.path.join(DATASET_PATH, 'train_data', 'data_list.csv')\n",
    "wav_paths = list()\n",
    "script_paths = list()\n",
    "\n",
    "with open(data_list, 'r') as f:\n",
    "    for line in f:\n",
    "        # line: \"aaa.wav,aaa.label\"\n",
    "        wav_path, script_path = line.strip().split(',') # wav_path 여기 있음!!\n",
    "        wav_paths.append(os.path.join(DATASET_PATH, 'train_data', wav_path))\n",
    "        script_paths.append(os.path.join(DATASET_PATH, 'train_data', script_path))\n",
    "\n",
    "best_loss = 1e10\n",
    "best_cer = 1.0\n",
    "begin_epoch = 0\n",
    "\n",
    "# load all target scripts for reducing disk i/o\n",
    "target_path = os.path.join(DATASET_PATH, 'train_label')\n",
    "load_targets(target_path, target_dict)\n",
    "\n",
    "# 데이터 로드 end\n",
    "train_batch_num, train_dataset_list, valid_dataset = split_dataset(hyper_p, wav_paths, script_paths, valid_ratio = 0.05, \n",
    "                                                                   target_dict = target_dict)\n",
    "\n",
    "\n",
    "logger.info('start')\n",
    "train_begin = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(begin_epoch, hyper_p.max_epochs):\n",
    "    train_queue = queue.Queue(hyper_p.workers * 2)\n",
    "\n",
    "    train_loader = MultiLoader(train_dataset_list, train_queue, hyper_p.batch_size, hyper_p.workers)\n",
    "    train_loader.start()\n",
    "\n",
    "    if epoch == 30:\n",
    "        optimizer = optim.Adam(model.module.parameters(), lr = 0.00005 )\n",
    "\n",
    "    train_loss, train_cer = train(model, train_batch_num,\n",
    "                                  train_queue, criterion,\n",
    "                                  optimizer, device,\n",
    "                                  train_begin, hyper_p.workers,\n",
    "                                  10, hyper_p.teacher_forcing)\n",
    "\n",
    "    logger.info('Epoch %d (Training) Loss %0.4f CER %0.4f' % (epoch, train_loss, train_cer))\n",
    "\n",
    "    train_loader.join()\n",
    "\n",
    "    valid_queue = queue.Queue(hyper_p.workers * 2)\n",
    "    valid_loader = BaseDataLoader(valid_dataset, valid_queue, hyper_p.batch_size, 0)\n",
    "    valid_loader.start()\n",
    "\n",
    "    eval_loss, eval_cer = evaluate(model, valid_loader, valid_queue, criterion, device)\n",
    "    logger.info('Epoch %d (Evaluate) Loss %0.4f CER %0.4f' % (epoch, eval_loss, eval_cer))\n",
    "\n",
    "    valid_loader.join()\n",
    "\n",
    "    best_model = (eval_loss < best_loss)\n",
    "    best_cer_model = (eval_cer < best_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
